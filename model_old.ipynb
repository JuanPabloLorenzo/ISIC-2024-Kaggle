{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from operator import itemgetter\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BASE_PATH = 'isic-2024-challenge/'\n",
    "    TRAIN_IMAGE_PATH = 'train-image.hdf5'\n",
    "    TRAIN_METADATA_PATH = 'train-metadata.csv'\n",
    "    TEST_IMAGE_PATH = 'test-image.hdf5'\n",
    "    TEST_METADATA_PATH = 'test-metadata.csv'\n",
    "    \n",
    "    # Data processing\n",
    "    IMAGE_SIZE = (120, 120)\n",
    "    VALIDATION_SPLIT = 0.1\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    \n",
    "    class MetadataModule:\n",
    "        ACTIVATION = 'relu'\n",
    "        KERNEL_INITIALIZER = 'he_normal'\n",
    "        \n",
    "    class ImageModule:\n",
    "        ACTIVATION = 'relu'\n",
    "        KERNEL_INITIALIZER = 'he_normal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bw/_7s8wxw93cngpt5f7bg5s3hm0000gn/T/ipykernel_19537/2340496625.py:4: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata = pd.read_csv(Config.BASE_PATH + Config.TRAIN_METADATA_PATH)\n"
     ]
    }
   ],
   "source": [
    "train_hdf5 = h5py.File(Config.BASE_PATH + Config.TRAIN_IMAGE_PATH, 'r')\n",
    "test_hdf5 = h5py.File(Config.BASE_PATH + Config.TEST_IMAGE_PATH, 'r')\n",
    "\n",
    "train_metadata = pd.read_csv(Config.BASE_PATH + Config.TRAIN_METADATA_PATH)\n",
    "test_metadata = pd.read_csv(Config.BASE_PATH + Config.TEST_METADATA_PATH)\n",
    "\n",
    "train_fnames = train_metadata[\"isic_id\"].tolist()\n",
    "test_fnames = test_metadata[\"isic_id\"].tolist()\n",
    "\n",
    "train_target = train_metadata[\"target\"]\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=Config.VALIDATION_SPLIT, random_state=Config.RANDOM_STATE)\n",
    "for train_index, val_index in split.split(train_metadata, train_target):\n",
    "    val_fnames = itemgetter(*val_index)(train_fnames)\n",
    "    train_fnames = itemgetter(*train_index)(train_fnames)\n",
    "    X_metadata_train, X_metadata_val = train_metadata.iloc[train_index], train_metadata.iloc[val_index]\n",
    "    y_train, y_val = train_target.iloc[train_index], train_target.iloc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_train_cols = [\"target\", \"lesion_id\", \"iddx_full\", \"iddx_1\", \"iddx_2\", \"iddx_3\", \"iddx_4\", \"iddx_5\", \"mel_mitotic_index\", \"mel_thick_mm\", \"tbp_lv_dnn_lesion_confidence\"]\n",
    "unuseful_cols = [\"image_type\", \"patient_id\"]\n",
    "removable_cols = only_train_cols + unuseful_cols + [\"isic_id\"]\n",
    "\n",
    "numeric_features = train_metadata.select_dtypes(include=['float64', 'int64']).columns.difference(removable_cols)\n",
    "cat_features = train_metadata.select_dtypes(include=['object']).columns.difference(removable_cols)\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ])\n",
    "\n",
    "metadata_preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "X_train_metadata_preprocessed = metadata_preprocessing_pipeline.fit_transform(X_metadata_train)\n",
    "X_val_metadata_preprocessed = metadata_preprocessing_pipeline.transform(X_metadata_val)\n",
    "X_test_metadata_preprocessed = metadata_preprocessing_pipeline.transform(test_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "train_target_ds = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "\n",
    "def load_train_image(id):\n",
    "    image = Image.open(io.BytesIO(np.array(train_hdf5[id.numpy()])))\n",
    "    image = np.array(image.resize(Config.IMAGE_SIZE)).reshape(120, 120, 3)\n",
    "    return image\n",
    "\n",
    "def load_test_image(id):\n",
    "    image = Image.open(io.BytesIO(np.array(test_hdf5[id.numpy()])))\n",
    "    image = np.array(image.resize(Config.IMAGE_SIZE)).reshape(120, 120, 3)\n",
    "    return image\n",
    "\n",
    "def set_shapes(image):\n",
    "    image.set_shape([120, 120, 3])\n",
    "    return image\n",
    "\n",
    "# Create a dataset for images\n",
    "train_image_ds = tf.data.Dataset.from_tensor_slices(tf.constant(train_fnames))\n",
    "train_image_ds = train_image_ds.map(lambda x: tf.py_function(load_train_image, [x], tf.float32))\n",
    "train_image_ds = train_image_ds.map(set_shapes)\n",
    "train_solo_image_ds = tf.data.Dataset.zip((train_image_ds, train_target_ds)).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create a dataset for metadata\n",
    "train_metadata_ds = tf.data.Dataset.from_tensor_slices(X_train_metadata_preprocessed)\n",
    "train_solo_metadata_ds = tf.data.Dataset.zip((train_metadata_ds, train_target_ds)).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Combine the datasets\n",
    "train_ds = tf.data.Dataset.zip(((train_image_ds, train_metadata_ds), train_target_ds))\n",
    "train_ds = train_ds.shuffle(1000).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# VAL\n",
    "val_target_ds = tf.data.Dataset.from_tensor_slices(y_val)\n",
    "\n",
    "# Create a dataset for images\n",
    "val_image_ds = tf.data.Dataset.from_tensor_slices(tf.constant(val_fnames))\n",
    "val_image_ds = val_image_ds.map(lambda x: tf.py_function(load_train_image, [x], tf.float32))\n",
    "val_image_ds = val_image_ds.map(set_shapes)\n",
    "val_solo_image_ds = tf.data.Dataset.zip((val_image_ds, val_target_ds)).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create a dataset for metadata\n",
    "val_metadata_ds = tf.data.Dataset.from_tensor_slices(X_val_metadata_preprocessed)\n",
    "val_solo_metadata_ds = tf.data.Dataset.zip((val_metadata_ds, val_target_ds)).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Combine the datasets\n",
    "val_ds = tf.data.Dataset.zip(((val_image_ds, val_metadata_ds), val_target_ds))\n",
    "val_ds = val_ds.batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# TEST\n",
    "test_image_ds = tf.data.Dataset.from_tensor_slices(tf.constant(test_fnames))\n",
    "test_image_ds = test_image_ds.map(lambda x: tf.py_function(load_test_image, [x], tf.float32))\n",
    "test_image_ds = test_image_ds.map(set_shapes)\n",
    "\n",
    "test_metadata_ds = tf.data.Dataset.from_tensor_slices(X_test_metadata_preprocessed)\n",
    "test_ds = tf.data.Dataset.zip((test_image_ds, test_metadata_ds)).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "def pauc_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = 1.0 - y_pred\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = 1 - min_tpr\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return partial_auc\n",
    "\n",
    "class PAUCCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        super(PAUCCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get predictions for validation data\n",
    "        val_pred = self.model.predict(self.validation_data, verbose=0)\n",
    "        \n",
    "        # Extract true labels from validation data\n",
    "        y_val = np.concatenate([y for x, y in self.validation_data], axis=0)\n",
    "        \n",
    "        # Calculate pAUC score\n",
    "        pauc = pauc_score(y_val, val_pred)\n",
    "        \n",
    "        # Optionally, you can add the pAUC score to the logs\n",
    "        logs['val_pauc'] = pauc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                5312      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7425 (29.00 KB)\n",
      "Trainable params: 7425 (29.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "metadata_input_shape = next(iter(train_metadata_ds.take(1))).shape\n",
    "\n",
    "metadata_model = Sequential([\n",
    "    Dense(64, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER, input_shape=metadata_input_shape),\n",
    "    Dense(32, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002)\n",
    "metadata_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "pauc_callback = PAUCCallback(val_solo_metadata_ds, Config.BATCH_SIZE)\n",
    "\n",
    "# Display model summary\n",
    "metadata_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11280/11280 [==============================] - 5s 455us/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.0062 - val_accuracy: 0.9990 - val_pauc: 0.1245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x31083df90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_model.fit(train_solo_metadata_ds, validation_data=val_solo_metadata_ds, epochs=1, callbacks=[pauc_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 59, 59, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                200768    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 206353 (806.07 KB)\n",
      "Trainable params: 206353 (806.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_input_shape = next(iter(train_image_ds.take(1))).shape\n",
    "\n",
    "image_model = Sequential([\n",
    "    Conv2D(32, 3, 2, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER, input_shape=image_input_shape),\n",
    "    Conv2D(16, 3, 2, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "image_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "pauc_callback = PAUCCallback(val_solo_image_ds.take(100), Config.BATCH_SIZE)\n",
    "\n",
    "# Display model summary\n",
    "image_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 20s 40ms/step - loss: 0.8098 - accuracy: 0.9961 - val_loss: 0.7174 - val_accuracy: 0.9991 - val_pauc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x36c917fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model.fit(train_solo_image_ds.take(500), validation_data=val_solo_image_ds.take(100), epochs=1, callbacks=[pauc_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = tf.keras.Input(shape=image_input_shape)\n",
    "metadata_input = tf.keras.Input(shape=metadata_input_shape)\n",
    "\n",
    "# Clone and freeze image model layers\n",
    "x_image = image_input\n",
    "for layer in image_model.layers[:-1]:  # Exclude the last layer\n",
    "    x_image = layer(x_image)\n",
    "    layer.trainable = False\n",
    "\n",
    "# Clone and freeze metadata model layers\n",
    "x_metadata = metadata_input\n",
    "for layer in metadata_model.layers[:-1]:  # Exclude the last layer\n",
    "    x_metadata = layer(x_metadata)\n",
    "    layer.trainable = False\n",
    "\n",
    "# Concatenate the outputs of both models\n",
    "combined = tf.keras.layers.Concatenate()([x_image, x_metadata])\n",
    "x = tf.keras.layers.Dense(16, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER)(combined)\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Define inputs\n",
    "input = [image_input, metadata_input]\n",
    "\n",
    "combined_model = tf.keras.Model(inputs=input, outputs=x)\n",
    "\n",
    "pauc_callback = PAUCCallback(val_ds.take(100), Config.BATCH_SIZE)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0002)\n",
    "combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 17s 32ms/step - loss: 0.0380 - accuracy: 0.9984 - val_loss: 0.0550 - val_accuracy: 0.9991 - val_pauc: 0.0200\n",
      "Layers unfrozen and model recompiled for fine-tuning.\n",
      "500/500 [==============================] - 15s 29ms/step - loss: 0.0408 - accuracy: 0.9991 - val_loss: 0.0479 - val_accuracy: 0.9991 - val_pauc: 0.0199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x391ff85d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.fit(train_ds.take(500), validation_data=val_ds.take(100), epochs=1, callbacks=[pauc_callback])\n",
    "\n",
    "# Unfreeze layers in the image model\n",
    "for layer in combined_model.layers:\n",
    "    if isinstance(layer, tf.keras.Model) and layer.name == image_model.name:\n",
    "        for sub_layer in layer.layers:\n",
    "            sub_layer.trainable = True\n",
    "\n",
    "# Unfreeze layers in the metadata model\n",
    "for layer in combined_model.layers:\n",
    "    if isinstance(layer, tf.keras.Model) and layer.name == metadata_model.name:\n",
    "        for sub_layer in layer.layers:\n",
    "            sub_layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate for fine-tuning\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.00001)\n",
    "combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Layers unfrozen and model recompiled for fine-tuning.\")\n",
    "\n",
    "combined_model.fit(train_ds.take(500), validation_data=val_ds.take(100), epochs=1, callbacks=[pauc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv(Config.BASE_PATH + 'sample_submission.csv')\n",
    "# Create a dataset of zeros with the same length as the test dataset\n",
    "zero_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(test_ds)))\n",
    "\n",
    "# Combine the test dataset with the zero labels\n",
    "test_ds_with_zeros = tf.data.Dataset.zip((test_ds, zero_labels))\n",
    "\n",
    "submission[\"target\"] = combined_model.predict(test_ds_with_zeros)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
