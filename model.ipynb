{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T20:42:19.402478Z","iopub.status.busy":"2024-07-26T20:42:19.402114Z","iopub.status.idle":"2024-07-26T20:42:33.666779Z","shell.execute_reply":"2024-07-26T20:42:33.665815Z","shell.execute_reply.started":"2024-07-26T20:42:19.402447Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import h5py\n","import io\n","from PIL import Image\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from operator import itemgetter\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T20:42:58.961831Z","iopub.status.busy":"2024-07-26T20:42:58.960674Z","iopub.status.idle":"2024-07-26T20:42:58.969769Z","shell.execute_reply":"2024-07-26T20:42:58.968305Z","shell.execute_reply.started":"2024-07-26T20:42:58.961789Z"},"trusted":true},"outputs":[],"source":["def is_kaggle():\n","    return os.path.exists('/kaggle')\n","\n","class Config:\n","    BASE_PATH = '/kaggle/input/isic-2024-challenge/' if is_kaggle() else 'isic-2024-challenge/'\n","    TRAIN_IMAGE_PATH = 'train-image.hdf5'\n","    TRAIN_METADATA_PATH = 'train-metadata.csv'\n","    TEST_IMAGE_PATH = 'test-image.hdf5'\n","    TEST_METADATA_PATH = 'test-metadata.csv'\n","    \n","    # Data processing\n","    IMAGE_SIZE = (120, 120)\n","    VALIDATION_SPLIT = 0.15\n","    RANDOM_STATE = 42\n","    \n","    BATCH_SIZE = 32\n","    \n","    class MetadataModule:\n","        ACTIVATION = 'relu'\n","        KERNEL_INITIALIZER = 'he_normal'\n","        \n","    class ImageModule:\n","        ACTIVATION = 'relu'\n","        KERNEL_INITIALIZER = 'he_normal'"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocesamiento"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T20:43:00.904974Z","iopub.status.busy":"2024-07-26T20:43:00.904260Z","iopub.status.idle":"2024-07-26T20:43:06.942299Z","shell.execute_reply":"2024-07-26T20:43:06.941367Z","shell.execute_reply.started":"2024-07-26T20:43:00.904940Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/bw/_7s8wxw93cngpt5f7bg5s3hm0000gn/T/ipykernel_22758/2340496625.py:4: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n","  train_metadata = pd.read_csv(Config.BASE_PATH + Config.TRAIN_METADATA_PATH)\n"]}],"source":["train_hdf5 = h5py.File(Config.BASE_PATH + Config.TRAIN_IMAGE_PATH, 'r')\n","test_hdf5 = h5py.File(Config.BASE_PATH + Config.TEST_IMAGE_PATH, 'r')\n","\n","train_metadata = pd.read_csv(Config.BASE_PATH + Config.TRAIN_METADATA_PATH)\n","test_metadata = pd.read_csv(Config.BASE_PATH + Config.TEST_METADATA_PATH)\n","\n","train_fnames = train_metadata[\"isic_id\"].tolist()\n","test_fnames = test_metadata[\"isic_id\"].tolist()\n","\n","train_target = train_metadata[\"target\"]\n","\n","split = StratifiedShuffleSplit(n_splits=1, test_size=Config.VALIDATION_SPLIT, random_state=Config.RANDOM_STATE)\n","for train_index, val_index in split.split(train_metadata, train_target):\n","    val_fnames = itemgetter(*val_index)(train_fnames)\n","    train_fnames = itemgetter(*train_index)(train_fnames)\n","    X_metadata_train, X_metadata_val = train_metadata.iloc[train_index], train_metadata.iloc[val_index]\n","    y_train, y_val = train_target.iloc[train_index], train_target.iloc[val_index]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T20:43:12.051636Z","iopub.status.busy":"2024-07-26T20:43:12.051255Z","iopub.status.idle":"2024-07-26T20:43:14.414852Z","shell.execute_reply":"2024-07-26T20:43:14.413866Z","shell.execute_reply.started":"2024-07-26T20:43:12.051607Z"},"trusted":true},"outputs":[],"source":["only_train_cols = [\"target\", \"lesion_id\", \"iddx_full\", \"iddx_1\", \"iddx_2\", \"iddx_3\", \"iddx_4\", \"iddx_5\", \"mel_mitotic_index\", \"mel_thick_mm\", \"tbp_lv_dnn_lesion_confidence\"]\n","unuseful_cols = [\"image_type\", \"patient_id\"]\n","removable_cols = only_train_cols + unuseful_cols + [\"isic_id\"]\n","\n","numeric_features = train_metadata.select_dtypes(include=['float64', 'int64']).columns.difference(removable_cols)\n","cat_features = train_metadata.select_dtypes(include=['object']).columns.difference(removable_cols)\n","\n","numeric_pipeline = Pipeline([\n","    ('impute', SimpleImputer(strategy='mean')),\n","    ('scale', StandardScaler())\n","])\n","\n","cat_pipeline = Pipeline([\n","    ('impute', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n","])\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_pipeline, numeric_features),\n","        ('cat', cat_pipeline, cat_features)\n","    ])\n","\n","metadata_preprocessing_pipeline = Pipeline([\n","    ('preprocessor', preprocessor)\n","])\n","\n","X_train_metadata_preprocessed = metadata_preprocessing_pipeline.fit_transform(X_metadata_train)\n","X_val_metadata_preprocessed = metadata_preprocessing_pipeline.transform(X_metadata_val)\n","X_test_metadata_preprocessed = metadata_preprocessing_pipeline.transform(test_metadata)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:44:08.739878Z","iopub.status.busy":"2024-07-26T22:44:08.739446Z","iopub.status.idle":"2024-07-26T22:44:09.631510Z","shell.execute_reply":"2024-07-26T22:44:09.630459Z","shell.execute_reply.started":"2024-07-26T22:44:08.739850Z"},"trusted":true},"outputs":[],"source":["def create_dataset(fnames, metadata_preprocessed, targets, hdf5):\n","    target_ds = tf.data.Dataset.from_tensor_slices(targets)\n","    \n","    def load_image(id):\n","        image = Image.open(io.BytesIO(np.array(hdf5[id.numpy()])))\n","        image = np.array(image.resize(Config.IMAGE_SIZE)).reshape(120, 120, 3)\n","        return image\n","\n","    # It doesn't work without this in Kaggle\n","    def set_shapes(image):\n","        image.set_shape([120, 120, 3])\n","        return image\n","\n","    # Create a dataset for images\n","    image_ds = tf.data.Dataset.from_tensor_slices(tf.constant(fnames))\n","    image_ds = image_ds.map(lambda x: tf.py_function(load_image, [x], tf.float32))\n","    image_ds = image_ds.map(set_shapes)\n","    solo_image_ds = tf.data.Dataset.zip((image_ds, target_ds)).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","\n","    # Create a dataset for metadata\n","    metadata_ds = tf.data.Dataset.from_tensor_slices(metadata_preprocessed)\n","    solo_metadata_ds = tf.data.Dataset.zip((metadata_ds, target_ds)).batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","\n","    # Combine the datasets\n","    combined_ds = tf.data.Dataset.zip(((image_ds, metadata_ds), target_ds))\n","    combined_ds = combined_ds.shuffle(1000)\n","    combined_ds = combined_ds.batch(Config.BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n","\n","    return solo_image_ds, solo_metadata_ds, combined_ds\n","\n","train_solo_image_ds, train_solo_metadata_ds, train_ds = create_dataset(train_fnames, X_train_metadata_preprocessed, y_train, train_hdf5)\n","val_solo_image_ds, val_solo_metadata_ds, val_ds = create_dataset(val_fnames, X_val_metadata_preprocessed, y_val, train_hdf5)\n","\n","# TEST\n","test_solo_image_ds, test_solo_metadata_ds, test_ds = create_dataset(test_fnames, X_test_metadata_preprocessed, np.zeros(len(test_fnames)), test_hdf5)"]},{"cell_type":"markdown","metadata":{},"source":["# Auxiliary functions and classes"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:44:09.634076Z","iopub.status.busy":"2024-07-26T22:44:09.633661Z","iopub.status.idle":"2024-07-26T22:44:09.644307Z","shell.execute_reply":"2024-07-26T22:44:09.643126Z","shell.execute_reply.started":"2024-07-26T22:44:09.634039Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import roc_curve, auc, roc_auc_score\n","\n","def pauc_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    v_gt = abs(y_true - 1)\n","    v_pred = 1.0 - y_pred\n","    min_tpr = 0.80\n","    max_fpr = 1 - min_tpr\n","    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n","    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n","    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n","    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n","    \n","    return partial_auc\n","\n","class PAUCCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, validation_data, batch_size):\n","        super(PAUCCallback, self).__init__()\n","        self.validation_data = validation_data\n","        self.batch_size = batch_size\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # Get predictions for validation data\n","        val_pred = self.model.predict(self.validation_data, verbose=0)\n","        \n","        # Extract true labels from validation data\n","        y_val = np.concatenate([y for x, y in self.validation_data], axis=0)\n","        \n","        # Calculate pAUC score\n","        pauc = pauc_score(y_val, val_pred)\n","        \n","        # Optionally, you can add the pAUC score to the logs\n","        logs['val_pauc'] = pauc"]},{"cell_type":"markdown","metadata":{},"source":["# Metadata module"]},{"cell_type":"code","execution_count":224,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:44:10.458427Z","iopub.status.busy":"2024-07-26T22:44:10.457539Z","iopub.status.idle":"2024-07-26T22:44:10.698902Z","shell.execute_reply":"2024-07-26T22:44:10.697727Z","shell.execute_reply.started":"2024-07-26T22:44:10.458393Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shuffle 1/3\n","Split 1/5\n","pAUC = 0.1690\n","Split 2/5\n","pAUC = 0.1218\n","Split 3/5\n","pAUC = 0.1505\n","Split 4/5\n","pAUC = 0.1493\n","Split 5/5\n","pAUC = 0.1570\n","Shuffle 2/3\n","Split 1/5\n","pAUC = 0.1690\n","Split 2/5\n","pAUC = 0.1218\n","Split 3/5\n","pAUC = 0.1505\n","Split 4/5\n","pAUC = 0.1493\n","Split 5/5\n","pAUC = 0.1570\n","Shuffle 3/3\n","Split 1/5\n","pAUC = 0.1690\n","Split 2/5\n","pAUC = 0.1218\n","Split 3/5\n","pAUC = 0.1505\n","Split 4/5\n","pAUC = 0.1493\n","Split 5/5\n","pAUC = 0.1570\n"]}],"source":["from sklearn.model_selection import KFold\n","\n","metadata_input_shape = next(iter(train_solo_metadata_ds.take(1)))[0].shape[1:]\n","\n","shuffles = 3\n","splits = 5\n","models = []\n","scores = []\n","\n","for i in range(shuffles):\n","    print(f\"Shuffle {i+1}/{shuffles}\")\n","    split = KFold(n_splits=splits, random_state=Config.RANDOM_STATE, shuffle=True)\n","    for j, (train_index, val_index) in enumerate(split.split(X_metadata_train, y_train), 1):\n","        print(f\"Split {j}/{splits}\")\n","        X_metadata_train_split, X_metadata_val_split = X_metadata_train.iloc[train_index], X_metadata_train.iloc[val_index]\n","        y_train_split, y_val_split = y_train.iloc[train_index], y_train.iloc[val_index]\n","        \n","        X_train_metadata_preprocessed_split = metadata_preprocessing_pipeline.fit_transform(X_metadata_train_split)\n","        X_val_metadata_preprocessed_split = metadata_preprocessing_pipeline.transform(X_metadata_val_split)\n","        \n","        _, train_solo_metadata_split_ds, _ = create_dataset(train_fnames, X_train_metadata_preprocessed_split, y_train_split, train_hdf5)\n","        _, val_solo_metadata_split_ds, _ = create_dataset(val_fnames, X_val_metadata_preprocessed_split, y_val_split, train_hdf5)\n","        \n","        # metadata_model = Sequential([\n","        #     Dense(64, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER, input_shape=metadata_input_shape),\n","        #     Dense(32, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER),\n","        #     Dense(1, activation='sigmoid')\n","        # ])\n","\n","        # # Compile the model\n","        # lr = 3e-4\n","        # optimizer = tf.keras.optimizers.Adam(learning_rate=lr) if is_kaggle() else tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n","        # metadata_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","        # # Callbacks\n","        # pauc_callback = PAUCCallback(val_solo_metadata_split_ds, Config.BATCH_SIZE)\n","        # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_pauc', patience=3, mode='max')\n","        \n","        # history = metadata_model.fit(train_solo_metadata_split_ds, validation_data=val_solo_metadata_split_ds, epochs=8, callbacks=[pauc_callback, early_stopping])\n","        \n","        xgb_model = xgboost.XGBClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1, max_depth=3)\n","        xgb_model.fit(X_train_metadata_preprocessed_split, y_train_split)\n","\n","        pred = xgb_model.predict_proba(X_val_metadata_preprocessed_split)\n","        score = pauc_score(y_val_split, pred[:, 1])\n","        print(f\"pAUC = {score:.4f}\\n\")\n","        \n","        scores.append(score)\n","        models.append(xgb_model)\n","                \n","        # scores.append(history.history['val_pauc'])\n","        # models.append(metadata_model)"]},{"cell_type":"code","execution_count":264,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Making predictions on the validation set:\n","0.17020042866409085\n"]}],"source":["# Get the indices of the sorted scores\n","sorted_indices = [i for i, _ in sorted(enumerate(scores), key=lambda x: x[1], reverse=True)]\n","\n","best_models = [models[i] for i in sorted_indices]\n","\n","print(\"Making predictions on the validation set:\")\n","\n","y_preds = []\n","for i, model in enumerate(best_models, 1):\n","    # Make predictions on the validation set\n","    y_pred = model.predict_proba(X_val_metadata_preprocessed)[:, 1]\n","    \n","    y_preds.append(y_pred)\n","\n","y_preds = np.array(y_preds)\n","\n","n = 15\n","final_pred = np.mean(y_preds[:n], axis=0)\n","print(pauc_score(y_val, final_pred))"]},{"cell_type":"code","execution_count":227,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'<' not supported between instances of 'XGBClassifier' and 'XGBClassifier'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[227], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m model_score_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(models, scores))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Sort the pairs based on the maximum pAUC score (assuming higher is better)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# sorted_pairs = sorted(model_score_pairs, key=lambda x: max(x[1]), reverse=True)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m sorted_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_score_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m best_models \u001b[38;5;241m=\u001b[39m [pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m sorted_pairs]\n\u001b[1;32m     12\u001b[0m best_scores \u001b[38;5;241m=\u001b[39m [pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m sorted_pairs]\n","\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'XGBClassifier' and 'XGBClassifier'"]}],"source":["best_models = []\n","best_scores = []\n","\n","# Combine models and scores into tuples for sorting\n","model_score_pairs = list(zip(models, scores))\n","\n","# Sort the pairs based on the maximum pAUC score (assuming higher is better)\n","# sorted_pairs = sorted(model_score_pairs, key=lambda x: max(x[1]), reverse=True)\n","sorted_pairs = sorted(model_score_pairs, key=lambda x: x[1], reverse=True)\n","\n","best_models = [pair[0] for pair in sorted_pairs]\n","best_scores = [pair[1] for pair in sorted_pairs]\n","\n","print(f\"Models sorted based on validation pAUC scores:\")\n","for i, (model, score) in enumerate(zip(best_models, best_scores), 1):\n","    print(f\"Model {i}: Max pAUC = {max(score):.4f}\")"]},{"cell_type":"code","execution_count":239,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Making predictions on the validation set:\n"]}],"source":["print(\"Making predictions on the validation set:\")\n","\n","y_preds = []\n","for i, model in enumerate(best_models, 1):\n","    # Make predictions on the validation set\n","    y_pred = model.predict(X_val_metadata_preprocessed)\n","    \n","    y_preds.append(y_pred)\n","\n","y_preds = np.array(y_preds)"]},{"cell_type":"code","execution_count":238,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.019999001663893505\n"]}],"source":["n = 1\n","final_pred = np.mean(y_preds[:n], axis=0)\n","print(pauc_score(y_val, final_pred))"]},{"cell_type":"code","execution_count":223,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"XGBClassifier.fit() missing 1 required positional argument: 'y'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[223], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# xgb_model = xgboost.XGBClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1, max_depth=3)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# xgb_model.fit(X_train_metadata_preprocessed, y_train)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# pred = xgb_model.predict_proba(X_val_metadata_preprocessed)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(pauc_score(y_val, pred[:, 1]))\u001b[39;00m\n\u001b[1;32m      9\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgboost\u001b[38;5;241m.\u001b[39mXGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mRANDOM_STATE, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_solo_metadata_split_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# pred = xgb_model.predict_proba(X_val_metadata_preprocessed)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(pauc_score(y_val, pred[:, 1]))\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: XGBClassifier.fit() missing 1 required positional argument: 'y'"]}],"source":["import xgboost\n","\n","# xgb_model = xgboost.XGBClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1, max_depth=3)\n","# xgb_model.fit(X_train_metadata_preprocessed, y_train)\n","\n","# pred = xgb_model.predict_proba(X_val_metadata_preprocessed)\n","# print(pauc_score(y_val, pred[:, 1]))\n","\n","xgb_model = xgboost.XGBClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1, max_depth=3)\n","xgb_model.fit(train_solo_metadata_split_ds)\n","\n","# pred = xgb_model.predict_proba(X_val_metadata_preprocessed)\n","# print(pauc_score(y_val, pred[:, 1]))"]},{"cell_type":"markdown","metadata":{},"source":["# Image Module"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:44:14.837260Z","iopub.status.busy":"2024-07-26T22:44:14.836864Z","iopub.status.idle":"2024-07-26T22:44:14.922152Z","shell.execute_reply":"2024-07-26T22:44:14.921116Z","shell.execute_reply.started":"2024-07-26T22:44:14.837229Z"},"trusted":true},"outputs":[],"source":["# image_input_shape = next(iter(train_image_ds.take(1))).shape\n","\n","# image_model = Sequential([\n","#     Conv2D(32, 3, 2, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER, input_shape=image_input_shape),\n","#     Conv2D(16, 3, 2, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER),\n","#     MaxPooling2D(2, 2),\n","#     Flatten(),\n","#     Dense(64, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER),\n","#     Dense(1, activation='sigmoid')\n","# ])\n","\n","# # Compile the model\n","# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","# image_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# # Callbacks\n","# pauc_callback = PAUCCallback(val_solo_image_ds.take(100), Config.BATCH_SIZE)\n","\n","# # Display model summary\n","# image_model.summary()\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:33:01.527468Z","iopub.status.busy":"2024-07-26T22:33:01.527071Z","iopub.status.idle":"2024-07-26T22:35:10.121988Z","shell.execute_reply":"2024-07-26T22:35:10.120705Z","shell.execute_reply.started":"2024-07-26T22:33:01.527436Z"},"trusted":true},"outputs":[],"source":["# image_model.fit(train_solo_image_ds.take(500), validation_data=val_solo_image_ds.take(100), epochs=1, callbacks=[pauc_callback])"]},{"cell_type":"markdown","metadata":{},"source":["# Combined Modules"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:44:24.243613Z","iopub.status.busy":"2024-07-26T22:44:24.243245Z","iopub.status.idle":"2024-07-26T22:44:24.279723Z","shell.execute_reply":"2024-07-26T22:44:24.278513Z","shell.execute_reply.started":"2024-07-26T22:44:24.243584Z"},"trusted":true},"outputs":[],"source":["# image_input = tf.keras.Input(shape=image_input_shape)\n","# metadata_input = tf.keras.Input(shape=metadata_input_shape)\n","\n","# # Clone and freeze image model layers\n","# # x_image = image_input\n","# # for layer in image_model.layers[:-1]:  # Exclude the last layer\n","# #     x_image = layer(x_image)\n","# #     layer.trainable = False\n","\n","# # Clone and freeze metadata model layers\n","# x_metadata = metadata_input\n","# for layer in metadata_model.layers[:-1]:  # Exclude the last layer\n","#     x_metadata = layer(x_metadata)\n","#     layer.trainable = False\n","\n","# # Concatenate the outputs of both models\n","# # combined = tf.keras.layers.Concatenate()([x_image, x_metadata])\n","# x = tf.keras.layers.Dense(16, activation=Config.MetadataModule.ACTIVATION, kernel_initializer=Config.MetadataModule.KERNEL_INITIALIZER)(x_metadata)\n","# x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","# # Define inputs\n","# input = [image_input, metadata_input]\n","\n","# combined_model = tf.keras.Model(inputs=input, outputs=x)\n","\n","# pauc_callback = PAUCCallback(val_ds.take(100), Config.BATCH_SIZE)\n","\n","# # Compile the model\n","# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n","# combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:44:24.535464Z","iopub.status.busy":"2024-07-26T22:44:24.534765Z","iopub.status.idle":"2024-07-26T22:44:28.237944Z","shell.execute_reply":"2024-07-26T22:44:28.236546Z","shell.execute_reply.started":"2024-07-26T22:44:24.535429Z"},"trusted":true},"outputs":[],"source":["# combined_model.fit(train_ds.take(500), validation_data=val_ds.take(100), epochs=1, callbacks=[pauc_callback])\n","\n","# # Unfreeze layers in the image model\n","# for layer in combined_model.layers:\n","#     if isinstance(layer, tf.keras.Model) and layer.name == image_model.name:\n","#         for sub_layer in layer.layers:\n","#             sub_layer.trainable = True\n","\n","# # Unfreeze layers in the metadata model\n","# for layer in combined_model.layers:\n","#     if isinstance(layer, tf.keras.Model) and layer.name == metadata_model.name:\n","#         for sub_layer in layer.layers:\n","#             sub_layer.trainable = True\n","\n","# # Recompile the model with a lower learning rate for fine-tuning\n","# optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n","# combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# print(\"Layers unfrozen and model recompiled for fine-tuning.\")\n","\n","# combined_model.fit(train_ds.take(500), validation_data=val_ds.take(100), epochs=1, callbacks=[pauc_callback])"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T22:52:11.781374Z","iopub.status.busy":"2024-07-26T22:52:11.780578Z","iopub.status.idle":"2024-07-26T22:52:12.089014Z","shell.execute_reply":"2024-07-26T22:52:12.087900Z","shell.execute_reply.started":"2024-07-26T22:52:11.781339Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'test_metadata_ds' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Config\u001b[38;5;241m.\u001b[39mBASE_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_submission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# zero_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(test_ds)))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# test_ds_with_zeros = tf.data.Dataset.zip((test_ds, zero_labels))\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m zero_labels \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(tf\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_metadata_ds\u001b[49m)))\n\u001b[1;32m      6\u001b[0m test_ds_with_zeros \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((test_metadata_ds, zero_labels))\u001b[38;5;241m.\u001b[39mbatch(Config\u001b[38;5;241m.\u001b[39mBATCH_SIZE)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# submission[\"target\"] = combined_model.predict(test_ds_with_zeros)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# submission[\"target\"] = metadata_model.predict(test_ds_with_zeros)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# submission[\"target\"] = xgb_model.predict_proba(X_test_metadata_preprocessed)[:, 1]\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'test_metadata_ds' is not defined"]}],"source":["submission = pd.read_csv(Config.BASE_PATH + 'sample_submission.csv')\n","\n","# zero_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(test_ds)))\n","# test_ds_with_zeros = tf.data.Dataset.zip((test_ds, zero_labels))\n","zero_labels = tf.data.Dataset.from_tensor_slices(tf.zeros(len(test_metadata_ds)))\n","test_ds_with_zeros = tf.data.Dataset.zip((test_metadata_ds, zero_labels)).batch(Config.BATCH_SIZE)\n","\n","# submission[\"target\"] = combined_model.predict(test_ds_with_zeros)\n","# submission[\"target\"] = metadata_model.predict(test_ds_with_zeros)\n","# submission[\"target\"] = xgb_model.predict_proba(X_test_metadata_preprocessed)[:, 1]\n","pred = np.zeros(len(test_metadata_ds))\n","for model in best_models:\n","    pred += model.predict(test_ds_with_zeros).reshape(-1)\n","submission[\"target\"] = pred\n","submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":9094797,"sourceId":63056,"sourceType":"competition"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
