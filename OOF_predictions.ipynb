{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.applications import EfficientNetV2B0\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import io\n",
    "from PIL import Image\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "def is_kaggle():\n",
    "    return os.path.exists('/kaggle')\n",
    "\n",
    "class Config:\n",
    "    BASE_PATH = '/kaggle/input/isic-2024-challenge/' if is_kaggle() else 'isic-2024-challenge/'\n",
    "    TRAIN_IMAGE_PATH = 'train-image.hdf5'\n",
    "    TRAIN_METADATA_PATH = 'train-metadata.csv'\n",
    "    TEST_IMAGE_PATH = 'test-image.hdf5'\n",
    "    TEST_METADATA_PATH = 'test-metadata.csv'\n",
    "    \n",
    "    # Data processing\n",
    "    IMAGE_SIZE = (80, 80)\n",
    "    RANDOM_STATE = 42\n",
    "    \n",
    "    BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "def create_model():\n",
    "    efficientNet = EfficientNetV2B0(weights='imagenet', pooling='avg', include_top=False)\n",
    "\n",
    "    image_input = tf.keras.Input(shape=(*Config.IMAGE_SIZE, 3))\n",
    "    x = efficientNet(image_input)\n",
    "    x = Dense(512, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs=image_input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 15:56:56.245021: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-08-08 15:56:56.245043: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-08-08 15:56:56.245051: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-08-08 15:56:56.245081: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-08 15:56:56.245097: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "image_models = []\n",
    "for fname in os.listdir('weights_cnn'):\n",
    "    model = create_model()\n",
    "    model.load_weights(f\"weights_cnn/{fname}\")\n",
    "    image_models.append(model)\n",
    "    \n",
    "lgbm_models = []\n",
    "for fname in os.listdir('lgbm_models'):\n",
    "    lgbm_models.append(joblib.load(f'lgbm_models/{fname}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    eps = 1e-6\n",
    "    df[\"lesion_size_ratio\"] = np.minimum(df[\"tbp_lv_minorAxisMM\"] / (df[\"clin_size_long_diam_mm\"] + eps), 1.015)\n",
    "    df[\"lesion_shape_index\"] = np.minimum(df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2 + eps), 0.093)\n",
    "    df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n",
    "    df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n",
    "    df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n",
    "    df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n",
    "    df[\"color_uniformity\"] = np.log1p(np.minimum(df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"], 1000))\n",
    "    df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n",
    "    df[\"perimeter_to_area_ratio\"] = np.minimum(df[\"tbp_lv_perimeterMM\"] / (df[\"tbp_lv_areaMM2\"] + eps), 6.02)\n",
    "    df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n",
    "    df[\"combined_anatomical_site\"] = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n",
    "    df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n",
    "    df[\"color_consistency\"] = np.minimum(df[\"tbp_lv_stdL\"] / (df[\"tbp_lv_Lext\"] + eps), 0.305)\n",
    "    \n",
    "    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n",
    "    df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n",
    "    df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n",
    "    df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n",
    "    df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n",
    "    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n",
    "    df[\"normalized_lesion_size\"] = np.minimum(df[\"clin_size_long_diam_mm\"] / (df[\"age_approx\"] + eps), 1.59)\n",
    "    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n",
    "    df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n",
    "    df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n",
    "    df[\"3d_lesion_orientation\"] = np.arctan2(df[\"tbp_lv_y\"], df[\"tbp_lv_x\"])\n",
    "    df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n",
    "    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n",
    "    df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n",
    "\n",
    "    # Taken from: https://www.kaggle.com/code/dschettler8845/isic-detect-skin-cancer-let-s-learn-together\n",
    "    df[\"color_variance_ratio\"] = np.minimum(df[\"tbp_lv_color_std_mean\"] / (df[\"tbp_lv_stdLExt\"] + eps), 7.94)\n",
    "    df[\"border_color_interaction\"] = df[\"tbp_lv_norm_border\"] * df[\"tbp_lv_norm_color\"]\n",
    "    df[\"size_color_contrast_ratio\"] = np.minimum(df[\"clin_size_long_diam_mm\"] / (df[\"tbp_lv_deltaLBnorm\"] + eps), 5.08)\n",
    "    df[\"age_normalized_nevi_confidence\"] = np.minimum(df[\"tbp_lv_nevi_confidence\"] / (df[\"age_approx\"] + eps), 9.42)\n",
    "    df[\"color_asymmetry_index\"] = df[\"tbp_lv_radial_color_std_max\"] * df[\"tbp_lv_symm_2axis\"]\n",
    "    df[\"3d_volume_approximation\"] = df[\"tbp_lv_areaMM2\"] * np.sqrt(df[\"tbp_lv_x\"]**2 + df[\"tbp_lv_y\"]**2 + df[\"tbp_lv_z\"]**2)\n",
    "    df[\"color_range\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs() + (df[\"tbp_lv_A\"] - df[\"tbp_lv_Aext\"]).abs() + (df[\"tbp_lv_B\"] - df[\"tbp_lv_Bext\"]).abs()\n",
    "    df[\"shape_color_consistency\"] = df[\"tbp_lv_eccentricity\"] * df[\"tbp_lv_color_std_mean\"]\n",
    "    df[\"border_length_ratio\"] = np.minimum(df[\"tbp_lv_perimeterMM\"] / (2 * np.pi * np.sqrt(df[\"tbp_lv_areaMM2\"] / np.pi) + eps), 2.64)\n",
    "    df[\"age_size_symmetry_index\"] = df[\"age_approx\"] * df[\"clin_size_long_diam_mm\"] * df[\"tbp_lv_symm_2axis\"]\n",
    "    # Until here.\n",
    "    \n",
    "    new_num_cols = [\n",
    "        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n",
    "        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n",
    "        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\n",
    "        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"color_consistency\",\n",
    "\n",
    "        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\", \n",
    "        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n",
    "        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n",
    "        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n",
    "        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\",\n",
    "        \n",
    "        \"color_variance_ratio\", \"border_color_interaction\", \"size_color_contrast_ratio\",\n",
    "        \"age_normalized_nevi_confidence\", \"color_asymmetry_index\", \"3d_volume_approximation\",\n",
    "        \"color_range\", \"shape_color_consistency\", \"border_length_ratio\", \"age_size_symmetry_index\",\n",
    "    ]\n",
    "    new_cat_cols = [\"combined_anatomical_site\"]\n",
    "    return df, new_num_cols, new_cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bw/_7s8wxw93cngpt5f7bg5s3hm0000gn/T/ipykernel_88504/2084290553.py:4: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata = pd.read_csv(Config.BASE_PATH + Config.TRAIN_METADATA_PATH)\n"
     ]
    }
   ],
   "source": [
    "train_hdf5 = h5py.File(Config.BASE_PATH + Config.TRAIN_IMAGE_PATH, 'r')\n",
    "test_hdf5 = h5py.File(Config.BASE_PATH + Config.TEST_IMAGE_PATH, 'r')\n",
    "\n",
    "train_metadata = pd.read_csv(Config.BASE_PATH + Config.TRAIN_METADATA_PATH)\n",
    "test_metadata = pd.read_csv(Config.BASE_PATH + Config.TEST_METADATA_PATH)\n",
    "\n",
    "# Add features\n",
    "X_metadata_train, new_num_cols, new_cat_cols = feature_engineering(train_metadata)\n",
    "X_metadata_test, _, _ = feature_engineering(test_metadata)\n",
    "\n",
    "fnames = train_metadata[\"isic_id\"]\n",
    "test_fnames = test_metadata[\"isic_id\"]\n",
    "\n",
    "target = train_metadata[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_train_cols = [\"target\", \"lesion_id\", \"iddx_full\", \"iddx_1\", \"iddx_2\", \"iddx_3\", \"iddx_4\", \"iddx_5\", \"mel_mitotic_index\", \"mel_thick_mm\", \"tbp_lv_dnn_lesion_confidence\"]\n",
    "unuseful_cols = [\"image_type\", \"patient_id\"]\n",
    "removable_cols = only_train_cols + unuseful_cols + [\"isic_id\"]\n",
    "\n",
    "numeric_features = X_metadata_train.select_dtypes(include=['float64', 'int64']).columns.difference(removable_cols)\n",
    "cat_features = X_metadata_train.select_dtypes(include=['object']).columns.difference(removable_cols)\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('cat', cat_pipeline, cat_features)\n",
    "    ])\n",
    "\n",
    "metadata_preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "X_train_metadata_preprocessed = metadata_preprocessing_pipeline.fit_transform(X_metadata_train)\n",
    "X_test_metadata_preprocessed = metadata_preprocessing_pipeline.transform(X_metadata_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauc_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = 1.0 - y_pred\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = 1 - min_tpr\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return partial_auc\n",
    "\n",
    "class PAUCCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super(PAUCCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get predictions for validation data\n",
    "        val_pred = self.model.predict(self.validation_data, verbose=0)\n",
    "        \n",
    "        # Extract true labels from validation data\n",
    "        y_val = np.concatenate([y for x, y in self.validation_data], axis=0)\n",
    "        \n",
    "        # Calculate pAUC score\n",
    "        pauc = pauc_score(y_val, val_pred)\n",
    "        \n",
    "        # Optionally, you can add the pAUC score to the logs\n",
    "        logs['val_pauc'] = pauc\n",
    "        \n",
    "def create_image_dataset(fnames, targets, hdf5):\n",
    "    target_ds = tf.data.Dataset.from_tensor_slices(targets)\n",
    "    \n",
    "    def load_image(id):\n",
    "        image = Image.open(io.BytesIO(np.array(hdf5[id.numpy()])))\n",
    "        image = np.array(image.resize(Config.IMAGE_SIZE))\n",
    "        image = (image - image.min()) / (image.max() - image.min())\n",
    "        return image\n",
    "\n",
    "    # It doesn't work without this in Kaggle\n",
    "    def set_shapes(image):\n",
    "        image.set_shape([*Config.IMAGE_SIZE, 3])\n",
    "        return image\n",
    "\n",
    "    # Create a dataset for images\n",
    "    image_ds = tf.data.Dataset.from_tensor_slices(tf.constant(fnames))\n",
    "    image_ds = image_ds.map(lambda x: tf.py_function(load_image, [x], tf.float32))\n",
    "    image_ds = image_ds.map(set_shapes)\n",
    "    solo_image_ds = tf.data.Dataset.zip((image_ds, target_ds))\n",
    "\n",
    "    return solo_image_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 15:57:07.557337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 15:57:08.959228: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 15:57:10.225075: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 15:57:11.434087: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 15:57:12.862201: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x391ab52d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "test_ds = create_image_dataset(test_fnames, np.zeros(len(test_fnames)), test_hdf5)\n",
    "test_ds = test_ds.batch(128).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_predictions = np.zeros((len(test_fnames), len(image_models)))\n",
    "\n",
    "for i, model in enumerate(image_models):\n",
    "    test_predictions[:, i] = model.predict(test_ds).ravel()\n",
    "\n",
    "test_predictions_mean = np.mean(test_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = X_test_metadata_preprocessed.copy()\n",
    "X_test_final = np.column_stack((X_test_final, test_predictions_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(Config.BASE_PATH + 'sample_submission.csv')\n",
    "\n",
    "for model in lgbm_models:\n",
    "    pred = model.predict_proba(X_test_final)[:, 1]\n",
    "    submission[\"target\"] += pred\n",
    "\n",
    "submission[\"target\"] /= len(lgbm_models)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
